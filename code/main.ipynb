{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import darts\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import NBEATSModel\n",
    "from darts.timeseries import TimeSeries\n",
    "from darts.utils.data import TrainingDataset\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timeseries(\n",
    "    file_path=Path(\"data\", \"LD2011_2014.txt\"),\n",
    "    freq=\"2H\",\n",
    "    start_time=\"2014-01-02 00:00:00\",\n",
    "    end_time=\"2014-09-01 00:00:00\",\n",
    "    components=[f\"MT_{i:03}\" for i in range(1, 51)],\n",
    ") -> TimeSeries:\n",
    "    df = pd.read_csv(file_path, sep=\";\", index_col=0, parse_dates=True, decimal=\",\")\n",
    "    # df = df.replace(0, np.nan)\n",
    "\n",
    "    df = df.loc[start_time:end_time, components]\n",
    "\n",
    "    df = df.resample(freq).mean()\n",
    "\n",
    "    ts = TimeSeries.from_dataframe(df, freq=freq)\n",
    "\n",
    "    ts = ts.astype(np.float32)\n",
    "\n",
    "    return ts\n",
    "\n",
    "\n",
    "ts = load_timeseries()\n",
    "# ts = fill_missing_values(ts, interpolate_kwargs=dict(method='time'))\n",
    "assert not ts.pd_dataframe(copy=False).isna().any(axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ts.pd_dataframe(copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "https://www.tensorflow.org/tutorials/structured_data/time_series#normalize_the_data\n",
    "\n",
    "> It is important to scale features before training a neural network. Normalization is a common way of doing this scaling: subtract the mean and divide by the standard deviation of each feature.\n",
    ">\n",
    "> The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.\n",
    ">\n",
    "> It's also arguable that the model shouldn't have access to future values in the training set when training, and that this normalization should be done using moving averages. That's not the focus of this tutorial, and the validation and test sets ensure that you get (somewhat) honest metrics. So, in the interest of simplicity this tutorial uses a simple average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70 - 10 - 10 split\n",
    "ts_train_original, ts_nontrain_original = ts.split_before(0.8)\n",
    "ts_val_original, ts_test_original = ts_nontrain_original.split_before(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = Scaler(MinMaxScaler())\n",
    "transformer = Scaler(StandardScaler())\n",
    "ts_train_transformed = transformer.fit_transform(ts_train_original)\n",
    "ts_val_transformed = transformer.transform(ts_val_original)\n",
    "ts_test_transformed = transformer.transform(ts_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_objective(trial: optuna.Trial) -> float:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | stacks        | ModuleList       | 39.1 M\n",
      "---------------------------------------------------\n",
      "39.1 M    Trainable params\n",
      "26.5 K    Non-trainable params\n",
      "39.1 M    Total params\n",
      "156.504   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 70/70 [00:18<00:00,  3.84it/s, train_loss=0.146, val_loss=1.240]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 70/70 [00:18<00:00,  3.84it/s, train_loss=0.146, val_loss=1.240]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NBEATSModel(generic_architecture=True, num_stacks=30, num_blocks=1, num_layers=4, layer_widths=256, expansion_coefficient_dim=5, trend_polynomial_degree=2, dropout=0.0, activation=ReLU, input_chunk_length=84, output_chunk_length=12, optimizer_kwargs={'lr': 0.001}, pl_trainer_kwargs={'accelerator': 'gpu', 'devices': [0]})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NBEATSModel(\n",
    "    input_chunk_length=7 * 12,  # 7 days\n",
    "    output_chunk_length=1 * 12,  # 1 day\n",
    "    optimizer_kwargs=dict(\n",
    "        lr=1e-3,\n",
    "    ),\n",
    "    pl_trainer_kwargs=dict(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "    ),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    series=ts_train_transformed,\n",
    "    val_series=ts_val_transformed,\n",
    "    epochs=10,\n",
    "    num_loader_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.72it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = transformer.inverse_transform(\n",
    "    model.predict(n=1 * 12, series=ts_test_transformed[: 7 * 12])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2014-08-15 16:00:00', freq='2H')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2014-08-15 16:00:00', freq='2H')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_test_original[7 * 12 : 8 * 12].end_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
