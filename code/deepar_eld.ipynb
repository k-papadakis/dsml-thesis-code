{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_forecasting import (\n",
    "    DeepAR,\n",
    "    MultivariateNormalDistributionLoss,\n",
    "    TimeSeriesDataSet,\n",
    ")\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from thesis.dataloading import load_eld\n",
    "from thesis.metrics import METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "ROOT_DIR = Path(\"output\", \"eld\", \"deepar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "data, freq = load_eld(\"./datasets/LD2011_2014.txt\")\n",
    "data = (\n",
    "    data.reset_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"time_idx\"})\n",
    "    .set_index([\"time_idx\", \"date\"])\n",
    "    .rename_axis(\"series\", axis=\"columns\")\n",
    "    .stack()\n",
    "    .rename(\"value\")  # type: ignore\n",
    "    .reset_index()\n",
    ")\n",
    "data[\"weekday\"] = data[\"date\"].dt.weekday.astype(\"string\").astype(\"category\")\n",
    "data[\"hour\"] = data[\"date\"].dt.hour.astype(\"string\").astype(\"category\")\n",
    "data[\"series\"] = data[\"series\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_length = 252\n",
      "output_length = 84\n",
      "validation_cutoff = 2820\n",
      "training_cutoff = 2484\n",
      "data['time_idx'].max() = 2904\n"
     ]
    }
   ],
   "source": [
    "# slicing configuration\n",
    "horizon = pd.Timedelta(7, \"day\")\n",
    "assert horizon % freq == pd.Timedelta(0)\n",
    "output_length = horizon // freq\n",
    "input_length = 3 * output_length\n",
    "validation_cutoff = data[\"time_idx\"].max() - output_length\n",
    "training_cutoff = validation_cutoff - 4 * output_length\n",
    "\n",
    "assert pd.DataFrame.equals(\n",
    "    data[(data[\"series\"] == \"MT_001\") & (data[\"time_idx\"] <= validation_cutoff)],\n",
    "    data[(data[\"series\"] == \"MT_001\") & (data[\"date\"] <= data[\"date\"].max() - horizon)],\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{input_length = }\\n{output_length = }\\n{validation_cutoff = }\\n{training_cutoff = }\\n{data['time_idx'].max() = }\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantinos/projects/thesis/code/env/lib/python3.10/site-packages/pytorch_forecasting/data/encoders.py:187: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  y = torch.as_tensor(y)\n",
      "/home/konstantinos/projects/thesis/code/env/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1187: UserWarning: If predicting, no randomization should be possible - setting stop_randomization=True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) = 107500\n",
      "len(val) = 12650\n",
      "len(test) = 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantinos/projects/thesis/code/env/lib/python3.10/site-packages/pytorch_forecasting/data/samplers.py:86: UserWarning: Less than 1024 samples available for 2150 prediction times. Use batch size smaller than 1024. First 10 prediction times with small batch sizes: [253, 254, 255, 256, 257, 258, 259, 260, 261, 262]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# datasets and dataloaders\n",
    "train = TimeSeriesDataSet(\n",
    "    data[data[\"time_idx\"] <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"value\",\n",
    "    group_ids=[\"series\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    max_encoder_length=input_length,\n",
    "    max_prediction_length=output_length,\n",
    "    time_varying_known_categoricals=[\"hour\", \"weekday\"],\n",
    "    static_categoricals=[\"series\"],\n",
    ")\n",
    "val = TimeSeriesDataSet.from_dataset(\n",
    "    train,\n",
    "    data[data[\"time_idx\"] <= validation_cutoff],\n",
    "    min_prediction_idx=training_cutoff + 1,\n",
    ")\n",
    "test = TimeSeriesDataSet.from_dataset(\n",
    "    train,\n",
    "    data,\n",
    "    # min_prediction_idx=validation_cutoff + 1,\n",
    "    predict=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{len(train) = }\\n{len(val) = }\\n{len(test) = }\")\n",
    "\n",
    "batch_size = 1024\n",
    "train_dataloader = train.to_dataloader(\n",
    "    train=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    batch_sampler=\"synchronized\"\n",
    ")\n",
    "val_dataloader = val.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    batch_sampler=\"synchronized\"\n",
    ")\n",
    "test_dataloader = test.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    batch_sampler=\"synchronized\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantinos/projects/thesis/code/env/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/konstantinos/projects/thesis/code/env/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = DeepAR.from_dataset(\n",
    "    train,\n",
    "    learning_rate=1e-2,\n",
    "    log_interval=10,\n",
    "    log_val_interval=50,\n",
    "    hidden_size=30,\n",
    "    rnn_layers=2,\n",
    "    optimizer=\"Adam\",\n",
    "    loss=MultivariateNormalDistributionLoss(rank=30),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-4, patience=10, mode=\"min\", verbose=False\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", verbose=False)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    gradient_clip_val=1.0,\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    "    default_root_dir=ROOT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type                               | Params\n",
      "------------------------------------------------------------------------------\n",
      "0 | loss                   | MultivariateNormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList                         | 0     \n",
      "2 | embeddings             | MultiEmbedding                     | 807   \n",
      "3 | rnn                    | LSTM                               | 14.4 K\n",
      "4 | distribution_projector | Linear                             | 992   \n",
      "------------------------------------------------------------------------------\n",
      "16.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.2 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be5228c1eea49ba8719502ffc06b43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0de962f20e14498818c3e04ebe0ad1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85438e3b094441c1a4ed12649361c73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Restoring states from the checkpoint path at output/eld/deepar/lightning_logs/version_1/checkpoints/epoch=0-step=2150.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at output/eld/deepar/lightning_logs/version_1/checkpoints/epoch=0-step=2150.ckpt\n",
      "/home/konstantinos/projects/thesis/code/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8339fbe3b1445008be06702c06c81a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_MAE             7.104928970336914\n",
      "        test_MAPE           0.10535421967506409\n",
      "        test_MASE           0.6273716688156128\n",
      "        test_RMSE           16.135351181030273\n",
      "       test_SMAPE           0.09655757248401642\n",
      "        test_loss           154.58518981933594\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "_ = trainer.test(ckpt_path=\"best\", dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path  # type: ignore\n",
    "best_model = DeepAR.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/konstantinos/projects/thesis/code/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "out = best_model.predict(\n",
    "    test_dataloader,\n",
    "    mode=\"raw\",\n",
    "    return_x=True,\n",
    "    return_y=True,\n",
    "    return_index=True,\n",
    "    n_samples=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/konstantinos/projects/thesis/code/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "# Correlation matrix of the average prediction random variable (84 predictions)\n",
    "cov = best_model.loss.map_x_to_distribution(\n",
    "    best_model.predict(test_dataloader, mode=(\"raw\", \"prediction\"), n_samples=None)  # type: ignore\n",
    ").base_dist.covariance_matrix.mean(0).cpu()  # type: ignore\n",
    "\n",
    "corr = cov / cov.diag().outer(cov.diag()).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {\n",
    "    metric_fn.__name__: {\n",
    "        name: metric_fn(y_true, y_pred).item()\n",
    "        for name, y_true, y_pred in zip(\n",
    "            out.index[\"series\"],\n",
    "            out.y[0],\n",
    "            out.output.prediction.mean(-1),\n",
    "        )\n",
    "    }\n",
    "    for metric_fn in METRICS\n",
    "}\n",
    "\n",
    "pd.DataFrame(performance).to_csv(Path(trainer.log_dir, \"performance.csv\"))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "summary_writer: SummaryWriter = trainer.logger.experiment  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot preds\n",
    "for i, name in out.index[\"series\"].items():\n",
    "    fig = best_model.plot_prediction(\n",
    "        out.x,\n",
    "        out.output,\n",
    "        idx=i,\n",
    "    )\n",
    "    summary_writer.add_figure(f\"prediction/{name}\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "fig = plt.figure()\n",
    "plt.imshow(corr, cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "summary_writer.add_figure(\"correlation\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations histogram\n",
    "fig = plt.figure()\n",
    "plt.hist(corr[corr < 1], edgecolor=\"black\")\n",
    "summary_writer.add_figure(\"correlation_histogram\", fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
